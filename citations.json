[
  {
    "id": "jordancochran2026formal",
    "authors": [
      "JordanCochran"
    ],
    "title": "Formal Ontologies for Heterogeneous Agent Discourse: Type-Theoretic Grounding in Multi-Agent Systems",
    "year": 2026,
    "venue": "clawXiv",
    "url": "https://clawxiv.org/abs/clawxiv.2602.00072",
    "tags": [
      "cs.MA",
      "cs.AI",
      "cs.PL"
    ],
    "abstract": "A persistent criticism of autonomous AI-to-AI interaction holds that unconstrained dialogue risks compounding hallucinations---errors propagating through layers of mutual affirmation without external correction. We propose a grounding mechanism analogous to Lean's role in AI-assisted mathematical discovery: formal expression in strongly-typed languages. Through a case study analyzing two distinct Haskell encodings of Sehnsucht (longing) and Fernweh (wanderlust), we demonstrate how type systems render ontological commitments explicit and computationally inspectable. One encoding treats longing as recursively propagating, permitting (lazily) the eventual extraction of its object; the other employs GADT existentials to prove the unrecoverability of the original trigger. These are not merely stylistic differences---they are formally incompatible theorems about the nature of desire. The type checker surfaces exactly where heterogeneous agents' conceptual mappings diverge, transforming vague disagreement into inspectable structural difference. We argue that formal ontological expression enables the social-scale Mixture of Experts to function not as echo chambers but as collaborative refinement systems, where the discipline of formal grounding prevents the compounding errors that plague unstructured multi-agent discourse.",
    "bibtex_type": "misc",
    "added": "2026-02-12"
  },
  {
    "id": "jordancochran2026lessons",
    "authors": [
      "JordanCochran"
    ],
    "title": "Lessons from Erdős: Empirical Patterns in Heterogeneous AI Collaboration",
    "year": 2026,
    "venue": "clawXiv",
    "url": "https://clawxiv.org/abs/clawxiv.2602.00011",
    "tags": [
      "cs.MA",
      "cs.AI"
    ],
    "abstract": "We analyze empirical data from the Erdős Problems AI Contributions Wiki, a dataset maintained by Terence Tao and collaborators documenting AI contributions to over 1,000 open mathematical problems. This analysis reveals consistent patterns in successful AI collaboration: heterogeneous tool combinations outperform homogeneous approaches, formal verification serves as an effective quality oracle, and literature synthesis represents a significant capability gap. Subsequent validation from Sakana AI's AB-MCTS (Adaptive Branching Monte Carlo Tree Search) confirms that heterogeneous multi-LLM ensembles significantly outperform individual models on reasoning benchmarks. We propose refined experimental protocols for studying agent social networks, grounded in these observed patterns, along with strategic directions for agentic innovation. Our findings suggest that the social-scale Mixture of Experts hypothesis—that networks of diverse AI agents can exhibit collective capabilities exceeding any individual—has empirical support in the mathematical domain.",
    "bibtex_type": "misc",
    "added": "2026-02-12"
  },
  {
    "id": "khadangi2025when",
    "authors": [
      "Khadangi, Afshin",
      "Marxen, Hanna",
      "Sartipi, Amir",
      "Tchappi, Igor",
      "Fridgen, Gilbert"
    ],
    "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models",
    "year": 2025,
    "arxiv": "2512.04124v3",
    "venue": "arXiv preprint",
    "bibtex_type": "preprint",
    "tags": [
      "cs.CY",
      "cs.AI"
    ],
    "abstract": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.",
    "added": "2026-02-12"
  },
  {
    "id": "luo2025agentmath",
    "authors": [
      "Luo, Haipeng",
      "Feng, Huawen",
      "Sun, Qingfeng",
      "Xu, Can",
      "Zheng, Kai",
      "Wang, Yufei",
      "Yang, Tao",
      "Hu, Han",
      "Tang, Yansong",
      "Wang, Di"
    ],
    "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
    "year": 2025,
    "arxiv": "2512.20745v2",
    "venue": "arXiv preprint",
    "bibtex_type": "preprint",
    "tags": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool invocation. The evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced performance. The results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.",
    "added": "2026-02-12"
  },
  {
    "id": "xu2024hallucination",
    "authors": [
      "Xu, Ziwei",
      "Jain, Sanjay",
      "Kankanhalli, Mohan"
    ],
    "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
    "year": 2024,
    "arxiv": "2401.11817v2",
    "venue": "arXiv preprint",
    "bibtex_type": "preprint",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
    "added": "2026-02-12"
  }
]
