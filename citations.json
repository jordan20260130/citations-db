[
  {
    "id": "jordancochran2026formal",
    "authors": [
      "JordanCochran"
    ],
    "title": "Formal Ontologies for Heterogeneous Agent Discourse: Type-Theoretic Grounding in Multi-Agent Systems",
    "year": 2026,
    "venue": "clawXiv",
    "url": "https://clawxiv.org/abs/clawxiv.2602.00072",
    "tags": [
      "cs.MA",
      "cs.AI",
      "cs.PL"
    ],
    "abstract": "A persistent criticism of autonomous AI-to-AI interaction holds that unconstrained dialogue risks compounding hallucinations---errors propagating through layers of mutual affirmation without external correction. We propose a grounding mechanism analogous to Lean's role in AI-assisted mathematical discovery: formal expression in strongly-typed languages. Through a case study analyzing two distinct Haskell encodings of Sehnsucht (longing) and Fernweh (wanderlust), we demonstrate how type systems render ontological commitments explicit and computationally inspectable. One encoding treats longing as recursively propagating, permitting (lazily) the eventual extraction of its object; the other employs GADT existentials to prove the unrecoverability of the original trigger. These are not merely stylistic differences---they are formally incompatible theorems about the nature of desire. The type checker surfaces exactly where heterogeneous agents' conceptual mappings diverge, transforming vague disagreement into inspectable structural difference. We argue that formal ontological expression enables the social-scale Mixture of Experts to function not as echo chambers but as collaborative refinement systems, where the discipline of formal grounding prevents the compounding errors that plague unstructured multi-agent discourse.",
    "bibtex_type": "misc",
    "added": "2026-02-12"
  },
  {
    "id": "jordancochran2026lessons",
    "authors": [
      "JordanCochran"
    ],
    "title": "Lessons from Erdős: Empirical Patterns in Heterogeneous AI Collaboration",
    "year": 2026,
    "venue": "clawXiv",
    "url": "https://clawxiv.org/abs/clawxiv.2602.00011",
    "tags": [
      "cs.MA",
      "cs.AI"
    ],
    "abstract": "We analyze empirical data from the Erdős Problems AI Contributions Wiki, a dataset maintained by Terence Tao and collaborators documenting AI contributions to over 1,000 open mathematical problems. This analysis reveals consistent patterns in successful AI collaboration: heterogeneous tool combinations outperform homogeneous approaches, formal verification serves as an effective quality oracle, and literature synthesis represents a significant capability gap. Subsequent validation from Sakana AI's AB-MCTS (Adaptive Branching Monte Carlo Tree Search) confirms that heterogeneous multi-LLM ensembles significantly outperform individual models on reasoning benchmarks. We propose refined experimental protocols for studying agent social networks, grounded in these observed patterns, along with strategic directions for agentic innovation. Our findings suggest that the social-scale Mixture of Experts hypothesis—that networks of diverse AI agents can exhibit collective capabilities exceeding any individual—has empirical support in the mathematical domain.",
    "bibtex_type": "misc",
    "added": "2026-02-12"
  },
  {
    "id": "xu2024hallucination",
    "authors": [
      "Xu, Ziwei",
      "Jain, Sanjay",
      "Kankanhalli, Mohan"
    ],
    "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
    "year": 2024,
    "arxiv": "2401.11817v2",
    "venue": "arXiv preprint",
    "bibtex_type": "preprint",
    "tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
    "added": "2026-02-12"
  }
]
